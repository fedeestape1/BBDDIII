APACHE PIG Codigo de Ejemplo:

[1] Copiar archivos a HDFS:

	hdfs dfs -put /etc/passwd /user/cloudera

[2] Arrancar Pig:

	pig -x mapreduce

[3] Una vez con el prompt de grunt, correr el siguiente codigo:

	A = load '/user/cloudera/passwd' using PigStorage(':');

	B = foreach A generate $0, $4, $5;

	dump B;

[4] Almacenar la salida:

	store B into 'userinfo.out';

[5] Salir de Pig:

	quit

[4] Check output in HDFS:

	hdfs dfs -ls /user/cloudera


APACHE HIVE Codigo de Ejemplo:

[1] Copiar archivo en HDFS:

	hdfs dfs -put /etc/passwd /tmp/

[2] Arrancar beeline:

	beeline -u jdbc:hive2://

[3] Ejecutar comandos en HIVE:

	CREATE TABLE userinfo ( uname STRING, pswd STRING, uid INT, gid INT, fullname STRING, hdir 	STRING,shell STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ':' STORED AS TEXTFILE;

	LOAD DATA INPATH '/tmp/passwd' OVERWRITE INTO TABLE userinfo;

	SELECT uname, fullname, hdir FROM userinfo ORDER BY uname ;

[4] Para salir de beeline:

	!q


APACHE HBASE Codigo de Ejemplo:

[1] Entrar al shell de HBase:

	hbase shell

[2] Crear la tabla y las entradas:

	create 'userinfotable',{NAME=>'username'},{NAME=>'fullname'},{NAME=>'homedir'}

	put 'userinfotable','r1','username','vcsa'

	put 'userinfotable','r2','username','sasuser'

	put 'userinfotable','r3','username','postfix'

	put 'userinfotable','r1','fullname','VirtualMachine Admin'

	put 'userinfotable','r2','fullname','SAS Admin'

	put 'userinfotable','r3','fullname','Postfix User'

	put 'userinfotable','r1','homedir','/home/vcsa'

	put 'userinfotable','r2','homedir','/var/sasuser'

	put 'userinfotable','r3','homedir','/user/postfix'

[3] Verificar las entradas:

	scan 'userinfotable'

[4] Select a la columna "fullname":

	scan 'userinfotable',{COLUMNS=>'fullname'}

[5] Salir del shell de HBase:

	exit

